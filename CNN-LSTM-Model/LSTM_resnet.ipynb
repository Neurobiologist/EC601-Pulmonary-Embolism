{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "from tqdm import tqdm as tqdm\n",
    "import cv2\n",
    "import pickle\n",
    "from shutil import copyfile\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Level\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudyLevelDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Kaggle PE dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, stage):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.pedataframe = pd.read_csv(csv_file)\n",
    "        \n",
    "        self.hdf5_filename = '/scratch/features.hdf5'\n",
    "        \n",
    "        self._generate_row_indices_list()\n",
    "        \n",
    "        self.stage = stage\n",
    "        \n",
    "        if self.stage == 'train':\n",
    "            self.start = 0\n",
    "            self.end = 2650\n",
    "        else:\n",
    "            self.start = 2660\n",
    "            self.end = 3500\n",
    "        \n",
    "        # 0 - 2650 train\n",
    "        # 2660 - 3500 valid\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Return number of 2D images. (Each CT slice is an independent image.)\"\"\"\n",
    "        #return len(self.pedataframe)\n",
    "        return self.end - self.start + 1\n",
    "        #return num_pos_samples_per_dataset + num_neg_samples_per_dataset\n",
    "        \n",
    "    def _generate_row_indices_list(self):\n",
    "        # group slice indices into studies\n",
    "        self.row_indices = [] # index into train df for every study\n",
    "        current_study_id = ''\n",
    "\n",
    "        for slice_index in range(len(self.pedataframe)):\n",
    "            new_study_id = self.pedataframe.StudyInstanceUID[slice_index]\n",
    "            if new_study_id != current_study_id:\n",
    "                self.row_indices.append(slice_index)\n",
    "                current_study_id = new_study_id\n",
    "            if slice_index % 100000 == 0:\n",
    "                print(slice_index)\n",
    "        \n",
    "    def __getitem__(self, study_index):\n",
    "        '''  '''\n",
    "        study_index = study_index + self.start\n",
    "        \n",
    "        h5py_file = h5py.File(self.hdf5_filename, \"r\")\n",
    "        \n",
    "        start_index = self.row_indices[study_index]\n",
    "        if study_index+1 < len(self.row_indices):\n",
    "            stop_index = self.row_indices[study_index+1]\n",
    "        else:\n",
    "            stop_index = len(self.pedataframe)\n",
    "            \n",
    "        # dim: seq_len x embedding_dim\n",
    "        x = np.ones((stop_index-start_index, 2048))\n",
    "        y = np.ones(stop_index-start_index)\n",
    "            \n",
    "        for slice_index in range(start_index, stop_index):\n",
    "            data_identifier = 'tensor(' + str(slice_index) + ')'\n",
    "            slice_embeddings = h5py_file[data_identifier][:]\n",
    "            x[slice_index-start_index,:] = slice_embeddings\n",
    "            \n",
    "            pe_present_on_image = float(int(self.pedataframe.pe_present_on_image[slice_index]))\n",
    "            y[slice_index-start_index] = pe_present_on_image\n",
    "        \n",
    "        h5py_file.close()\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_filename = '/scratch/features.hdf5'\n",
    "h5py_file = h5py.File(hdf5_filename, \"r\")\n",
    "h5py_file['tensor(12)'][:]\n",
    "\n",
    "h5py_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/projectnb/ece601/kaggle-pulmonary-embolism/rsna-str-pulmonary-embolism-detection/'\n",
    "train_csv = data_dir + 'train.csv'\n",
    "train_dir = data_dir + 'train/'\n",
    "train_dataset = StudyLevelDataset(csv_file=train_csv, stage='train')\n",
    "valid_dataset = StudyLevelDataset(csv_file=train_csv, stage='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e\n",
    "'''\n",
    "\n",
    "batch_size = 1\n",
    "embedding_dim = 2048\n",
    "nb_lstm_units = 2048\n",
    "\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, nb_layers=1, nb_lstm_units=nb_lstm_units, \n",
    "                 embedding_dim=embedding_dim, batch_size=batch_size):\n",
    "        \n",
    "        super(MyLSTM, self).__init__()\n",
    "\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # output layer\n",
    "        self.linear = nn.Linear(self.nb_lstm_units, 1)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_h0 = torch.randn(self.nb_layers, self.batch_size, self.nb_lstm_units)\n",
    "        hidden_c0 = torch.randn(self.nb_layers, self.batch_size, self.nb_lstm_units)\n",
    "\n",
    "        hidden_h0 = hidden_h0.to(device)\n",
    "        hidden_c0 = hidden_c0.to(device)\n",
    "\n",
    "        hidden_h0 = Variable(hidden_h0)\n",
    "        hidden_c0 = Variable(hidden_c0)\n",
    "\n",
    "        return (hidden_h0.to(device), hidden_c0.to(device))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch. Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        #X = torch.nn.utils.rnn.pack_padded_sequence(x, X_lengths, batch_first=True)\n",
    "\n",
    "        # now run through LSTM\n",
    "        X, self.hidden = self.lstm(X, self.hidden)\n",
    "\n",
    "        # undo the packing operation\n",
    "        #X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        # this one is a bit tricky as well. First we need to reshape the data so it goes into the linear layer\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "\n",
    "        # run through actual linear layer\n",
    "        X = self.linear(X)\n",
    "\n",
    "        # ---------------------\n",
    "        # 4. Create softmax activations bc we're doing classification\n",
    "        # Dim transformation: (batch_size * seq_len, nb_lstm_units) -> (batch_size, seq_len, nb_tags)\n",
    "        #X = torch.sigmoid(X)\n",
    "\n",
    "        # I like to reshape for mental sanity so we're back to (batch_size, seq_len, nb_tags)\n",
    "        X = X.view(batch_size, seq_len)\n",
    "\n",
    "        Y_hat = X\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLSTM(\n",
       "  (lstm): LSTM(2048, 2048, batch_first=True)\n",
       "  (linear): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyLSTM()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model.to(device)\n",
    "x,y = dataset[1]\n",
    "x = torch.tensor(x)\n",
    "x = x.unsqueeze(0).float()\n",
    "x = x.to(device)\n",
    "\n",
    "pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  2 22:03:49 2020 Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.58490, mean: 0.11742: 100%|██████████| 2651/2651 [03:15<00:00, 13.56it/s]\n",
      "100%|██████████| 841/841 [01:00<00:00, 13.81it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  2 22:08:06 2020 Epoch 1, lr: 0.1000000, train loss: 0.11742, valid loss: 0.11713\n",
      "pos loss: 1.06458, neg loss: 0.06358, pos mean: 0.47428, neg mean 0.05167\n",
      "Wed Dec  2 22:08:06 2020 Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.56536, mean: 0.11430: 100%|██████████| 2651/2651 [03:09<00:00, 14.00it/s]\n",
      "100%|██████████| 841/841 [00:59<00:00, 14.09it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  2 22:12:15 2020 Epoch 2, lr: 0.1000000, train loss: 0.11430, valid loss: 0.11619\n",
      "pos loss: 1.02854, neg loss: 0.06458, pos mean: 0.48526, neg mean 0.05239\n",
      "Wed Dec  2 22:12:15 2020 Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.02645, mean: 0.15282:  11%|█         | 296/2651 [00:18<02:30, 15.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-f67b2458c292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-f67b2458c292>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "def criterion(logits, target):\n",
    "    loss = bce(logits.view(-1), target.view(-1))\n",
    "    return loss\n",
    "\n",
    "dummy = None\n",
    "\n",
    "def train_epoch(model, loader, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader)\n",
    "    for (data, target) in bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward pass\n",
    "        logits = model(data.float())\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        # backpropagate the loss (backward pass)\n",
    "        loss.backward()\n",
    " \n",
    "        # update parameters based on accumulated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "\n",
    "        train_loss.append(loss_np)\n",
    "        average_loss = sum(train_loss) / len(train_loss)\n",
    "        bar.set_description('loss: %.5f, mean: %.5f' % (loss_np, average_loss))\n",
    "\n",
    "    return float(average_loss)\n",
    "\n",
    "def valid_epoch(model, loader):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    pos_logits = []\n",
    "    neg_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in tqdm(loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits = model(data.float())\n",
    "\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            loss_np = loss.detach().cpu().numpy()\n",
    "\n",
    "            val_loss.append(float(loss_np))\n",
    "\n",
    "            logits = logits.squeeze()\n",
    "            target = target.squeeze()\n",
    "            \n",
    "            for i in range(logits.shape[-1]):\n",
    "                b = target[i].detach().cpu().numpy()\n",
    "                if b == 1:\n",
    "                    pos_logits.append(float(logits[i].detach().cpu().numpy()))\n",
    "                else:\n",
    "                    neg_logits.append(float(logits[i].detach().cpu().numpy()))\n",
    "                    \n",
    "    val_loss_mean = sum(val_loss) / len(val_loss)\n",
    "    \n",
    "    neg_logits_tensor = torch.FloatTensor(neg_logits).cuda()\n",
    "    pos_logits_tensor = torch.FloatTensor(pos_logits).cuda()\n",
    "    \n",
    "    neg_loss = criterion(neg_logits_tensor, torch.zeros(neg_logits_tensor.shape).float().cuda())\n",
    "    pos_loss = criterion(pos_logits_tensor, torch.ones(pos_logits_tensor.shape).float().cuda())\n",
    "    \n",
    "    neg_loss = neg_loss.detach().cpu().numpy()\n",
    "    pos_loss = pos_loss.detach().cpu().numpy()\n",
    "    \n",
    "    neg_mean = float(torch.sigmoid(neg_logits_tensor).mean().detach().cpu().numpy())\n",
    "    pos_mean = float(torch.sigmoid(pos_logits_tensor).mean().detach().cpu().numpy())\n",
    "    \n",
    "    return float(val_loss_mean), float(pos_loss), float(neg_loss), pos_mean, neg_mean\n",
    "\n",
    "def get_optimizer(lr, model):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0001)\n",
    "    return optimizer\n",
    "\n",
    "init_lr = 0.1\n",
    "n_epochs = 20\n",
    "device = 'cuda'\n",
    "\n",
    "# reduce LR by gamma three times every 10 epochs\n",
    "# each epoch is 100,000 samples\n",
    "#gamma = 10\n",
    "#schedule = [10, 20, 30]\n",
    "\n",
    "#model = resnext101.to(device)\n",
    "optimizer = get_optimizer(init_lr, model)\n",
    "model.to(device)\n",
    "\n",
    "master_train_loss = []\n",
    "master_valid_loss = []\n",
    "epoch = 1\n",
    "#best_valid_loss = 10\n",
    "\n",
    "\n",
    "while epoch <= n_epochs:\n",
    "    print(time.ctime(), 'Epoch:', epoch)\n",
    "    \n",
    "    # update learning rate \n",
    "    #if epoch in schedule:\n",
    "    #    new_lr = optimizer.param_groups[0][\"lr\"] / gamma\n",
    "     #   optimizer = get_optimizer(new_lr, model)\n",
    "    \n",
    "    #train_loader, valid_loader = get_loaders(epoch)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1,num_workers=1)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1,num_workers=1)\n",
    "\n",
    "    # train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # validate\n",
    "    valid_loss, pos_loss, neg_loss, pos_mean, neg_mean = valid_epoch(model, valid_loader)\n",
    "    #valid_loss = 0\n",
    "    #pos_loss  = 0\n",
    "    #neg_loss = 0\n",
    "    #pos_mean = 0\n",
    "    #neg_mean = 0\n",
    "    \n",
    "    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, valid loss: {(valid_loss):.5f}'\n",
    "    print(content)\n",
    "    content = f'pos loss: {(pos_loss):.5f}, neg loss: {(neg_loss):.5f}, pos mean: {(pos_mean):.5f}, neg mean {(neg_mean):.5f}'\n",
    "    print(content)\n",
    "    master_train_loss.append(train_loss)\n",
    "    master_valid_loss.append(valid_loss)\n",
    "    \n",
    "    # save loss data and model weights\n",
    "    #with open('train_loss.pkl', 'wb') as f:\n",
    "        #pickle.dump(master_train_loss, f)\n",
    "    #with open('valid_loss.pkl', 'wb') as f:\n",
    "        #pickle.dump(master_valid_loss, f)\n",
    "        \n",
    "        #torch.save(model.state_dict(), 'model-resnext-50-{}.pth'.format(epoch))\n",
    "        #best_valid_loss = valid_loss\n",
    "    \n",
    "    epoch += 1\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study Level Metrics\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudyLevelDataset2(torch.utils.data.Dataset):\n",
    "    \"\"\"Kaggle PE dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, stage):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.pedataframe = pd.read_csv(csv_file)\n",
    "        \n",
    "        self.hdf5_filename = '/scratch/features.hdf5'\n",
    "        \n",
    "        self._generate_row_indices_list()\n",
    "        \n",
    "        self.stage = stage\n",
    "        \n",
    "        if self.stage == 'train':\n",
    "            self.start = 0\n",
    "            self.end = 2650\n",
    "        else:\n",
    "            self.start = 2660\n",
    "            self.end = 3500\n",
    "        \n",
    "        # 0 - 2650 train\n",
    "        # 2660 - 3500 valid\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Return number of 2D images. (Each CT slice is an independent image.)\"\"\"\n",
    "        #return len(self.pedataframe)\n",
    "        return self.end - self.start + 1\n",
    "        #return num_pos_samples_per_dataset + num_neg_samples_per_dataset\n",
    "        \n",
    "    def _generate_row_indices_list(self):\n",
    "        # group slice indices into studies\n",
    "        self.row_indices = [] # index into train df for every study\n",
    "        current_study_id = ''\n",
    "\n",
    "        for slice_index in range(len(self.pedataframe)):\n",
    "            new_study_id = self.pedataframe.StudyInstanceUID[slice_index]\n",
    "            if new_study_id != current_study_id:\n",
    "                self.row_indices.append(slice_index)\n",
    "                current_study_id = new_study_id\n",
    "            if slice_index % 100000 == 0:\n",
    "                print(slice_index)\n",
    "        \n",
    "    def __getitem__(self, study_index):\n",
    "        '''  '''\n",
    "        study_index = study_index + self.start\n",
    "        \n",
    "        h5py_file = h5py.File(self.hdf5_filename, \"r\")\n",
    "        \n",
    "        start_index = self.row_indices[study_index]\n",
    "        if study_index+1 < len(self.row_indices):\n",
    "            stop_index = self.row_indices[study_index+1]\n",
    "        else:\n",
    "            stop_index = len(self.pedataframe)\n",
    "            \n",
    "        # dim: seq_len x embedding_dim\n",
    "        x = np.ones((stop_index-start_index, 2048))\n",
    "            \n",
    "        for slice_index in range(start_index, stop_index):\n",
    "            data_identifier = 'tensor(' + str(slice_index) + ')'\n",
    "            slice_embeddings = h5py_file[data_identifier][:]\n",
    "            x[slice_index-start_index,:] = slice_embeddings\n",
    "\n",
    "            \n",
    "        y = np.ones(9)\n",
    "            \n",
    "        y[0] =  self.pedataframe.negative_exam_for_pe[start_index]\n",
    "        y[1] = self.pedataframe.rv_lv_ratio_gte_1[start_index]\n",
    "        y[2] = self.pedataframe.rv_lv_ratio_lt_1[start_index]\n",
    "        y[3] = self.pedataframe.leftsided_pe[start_index]\n",
    "        y[4] = self.pedataframe.chronic_pe[start_index]\n",
    "        y[5] = self.pedataframe.rightsided_pe[start_index]\n",
    "        y[6] = self.pedataframe.acute_and_chronic_pe[start_index]\n",
    "        y[7] = self.pedataframe.central_pe[start_index]\n",
    "        y[8] = self.pedataframe.indeterminate[start_index]\n",
    "        \n",
    "        h5py_file.close()\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/projectnb/ece601/kaggle-pulmonary-embolism/rsna-str-pulmonary-embolism-detection/'\n",
    "train_csv = data_dir + 'train.csv'\n",
    "train_dir = data_dir + 'train/'\n",
    "train_dataset = StudyLevelDataset2(csv_file=train_csv, stage='train')\n",
    "valid_dataset = StudyLevelDataset2(csv_file=train_csv, stage='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e\n",
    "'''\n",
    "\n",
    "batch_size = 1\n",
    "embedding_dim = 2048\n",
    "nb_lstm_units = 2048\n",
    "\n",
    "class StudyLevelLSTM(nn.Module):\n",
    "    def __init__(self, nb_layers=1, nb_lstm_units=nb_lstm_units, \n",
    "                 embedding_dim=embedding_dim, batch_size=batch_size):\n",
    "        \n",
    "        super(StudyLevelLSTM, self).__init__()\n",
    "\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # output layer\n",
    "        self.linear = nn.Linear(self.nb_lstm_units, 9)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_h0 = torch.randn(self.nb_layers, self.batch_size, self.nb_lstm_units)\n",
    "        hidden_c0 = torch.randn(self.nb_layers, self.batch_size, self.nb_lstm_units)\n",
    "\n",
    "        hidden_h0 = hidden_h0.to(device)\n",
    "        hidden_c0 = hidden_c0.to(device)\n",
    "\n",
    "        hidden_h0 = Variable(hidden_h0)\n",
    "        hidden_c0 = Variable(hidden_c0)\n",
    "\n",
    "        return (hidden_h0.to(device), hidden_c0.to(device))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch. Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        #X = torch.nn.utils.rnn.pack_padded_sequence(x, X_lengths, batch_first=True)\n",
    "\n",
    "        # now run through LSTM\n",
    "        X, self.hidden = self.lstm(X, self.hidden)\n",
    "        \n",
    "        X = self.hidden[0]\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "\n",
    "        # run through actual linear layer\n",
    "        X = self.linear(X)\n",
    "\n",
    "        # I like to reshape for mental sanity so we're back to (batch_size, seq_len, nb_tags)\n",
    "        X = X.view(batch_size, 9)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StudyLevelLSTM(\n",
      "  (lstm): LSTM(2048, 2048, batch_first=True)\n",
      "  (linear): Linear(in_features=2048, out_features=9, bias=True)\n",
      ")\n",
      "tensor([[ 0.0113,  0.0154, -0.0072, -0.0215, -0.0083, -0.0220, -0.0144,  0.0094,\n",
      "         -0.0046]], grad_fn=<ViewBackward>)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# sanity test\n",
    "\n",
    "model = StudyLevelLSTM()\n",
    "print(model)\n",
    "device = 'cpu'\n",
    "model.to(device)\n",
    "x,y = train_dataset[1]\n",
    "x = torch.tensor(x)\n",
    "x = x.unsqueeze(0).float()\n",
    "x = x.to(device)\n",
    "\n",
    "pred = model(x)\n",
    "print(pred)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  2 22:39:18 2020 Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.85485, mean: 0.33492: 100%|██████████| 2651/2651 [02:32<00:00, 17.37it/s]\n",
      "100%|██████████| 841/841 [00:46<00:00, 17.98it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  2 22:42:38 2020 Epoch 1, lr: 0.1000000, train loss: 0.33492, valid loss: 0.34182\n",
      "pos loss: 0.98099, neg loss: 0.21307, pos mean: 0.43019, neg mean 0.17523\n",
      "Wed Dec  2 22:42:38 2020 Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.52156, mean: 0.33556:  35%|███▌      | 932/2651 [00:55<01:42, 16.82it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-da9b2cd41626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-da9b2cd41626>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "def criterion(logits, target):\n",
    "    loss = bce(logits.view(-1), target.view(-1))\n",
    "    return loss\n",
    "\n",
    "def train_epoch(model, loader, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader)\n",
    "    for (data, target) in bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward pass\n",
    "        logits = model(data.float())\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        # backpropagate the loss (backward pass)\n",
    "        loss.backward()\n",
    " \n",
    "        # update parameters based on accumulated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "\n",
    "        train_loss.append(loss_np)\n",
    "        average_loss = sum(train_loss) / len(train_loss)\n",
    "        bar.set_description('loss: %.5f, mean: %.5f' % (loss_np, average_loss))\n",
    "\n",
    "    return float(average_loss)\n",
    "\n",
    "def valid_epoch(model, loader):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    pos_logits = []\n",
    "    neg_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in tqdm(loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits = model(data.float())\n",
    "\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            loss_np = loss.detach().cpu().numpy()\n",
    "\n",
    "            val_loss.append(float(loss_np))\n",
    "\n",
    "            logits = logits.squeeze()\n",
    "            target = target.squeeze()\n",
    "            \n",
    "            for i in range(logits.shape[-1]):\n",
    "                b = target[i].detach().cpu().numpy()\n",
    "                if b == 1:\n",
    "                    pos_logits.append(float(logits[i].detach().cpu().numpy()))\n",
    "                else:\n",
    "                    neg_logits.append(float(logits[i].detach().cpu().numpy()))\n",
    "                    \n",
    "    val_loss_mean = sum(val_loss) / len(val_loss)\n",
    "    \n",
    "    neg_logits_tensor = torch.FloatTensor(neg_logits).cuda()\n",
    "    pos_logits_tensor = torch.FloatTensor(pos_logits).cuda()\n",
    "    \n",
    "    neg_loss = criterion(neg_logits_tensor, torch.zeros(neg_logits_tensor.shape).float().cuda())\n",
    "    pos_loss = criterion(pos_logits_tensor, torch.ones(pos_logits_tensor.shape).float().cuda())\n",
    "    \n",
    "    neg_loss = neg_loss.detach().cpu().numpy()\n",
    "    pos_loss = pos_loss.detach().cpu().numpy()\n",
    "    \n",
    "    neg_mean = float(torch.sigmoid(neg_logits_tensor).mean().detach().cpu().numpy())\n",
    "    pos_mean = float(torch.sigmoid(pos_logits_tensor).mean().detach().cpu().numpy())\n",
    "    \n",
    "    return float(val_loss_mean), float(pos_loss), float(neg_loss), pos_mean, neg_mean\n",
    "\n",
    "def get_optimizer(lr, model):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0001)\n",
    "    return optimizer\n",
    "\n",
    "init_lr = 0.1\n",
    "n_epochs = 20\n",
    "device = 'cuda'\n",
    "\n",
    "# reduce LR by gamma three times every 10 epochs\n",
    "# each epoch is 100,000 samples\n",
    "#gamma = 10\n",
    "#schedule = [10, 20, 30]\n",
    "\n",
    "#model = resnext101.to(device)\n",
    "optimizer = get_optimizer(init_lr, model)\n",
    "model.to(device)\n",
    "\n",
    "master_train_loss = []\n",
    "master_valid_loss = []\n",
    "epoch = 1\n",
    "#best_valid_loss = 10\n",
    "\n",
    "\n",
    "while epoch <= n_epochs:\n",
    "    print(time.ctime(), 'Epoch:', epoch)\n",
    "    \n",
    "    # update learning rate \n",
    "    #if epoch in schedule:\n",
    "    #    new_lr = optimizer.param_groups[0][\"lr\"] / gamma\n",
    "     #   optimizer = get_optimizer(new_lr, model)\n",
    "    \n",
    "    #train_loader, valid_loader = get_loaders(epoch)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1,num_workers=1)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1,num_workers=1)\n",
    "\n",
    "    # train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # validate\n",
    "    valid_loss, pos_loss, neg_loss, pos_mean, neg_mean = valid_epoch(model, valid_loader)\n",
    "    #valid_loss = 0\n",
    "    #pos_loss  = 0\n",
    "    #neg_loss = 0\n",
    "    #pos_mean = 0\n",
    "    #neg_mean = 0\n",
    "    \n",
    "    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, valid loss: {(valid_loss):.5f}'\n",
    "    print(content)\n",
    "    content = f'pos loss: {(pos_loss):.5f}, neg loss: {(neg_loss):.5f}, pos mean: {(pos_mean):.5f}, neg mean {(neg_mean):.5f}'\n",
    "    print(content)\n",
    "    master_train_loss.append(train_loss)\n",
    "    master_valid_loss.append(valid_loss)\n",
    "    \n",
    "    # save loss data and model weights\n",
    "    #with open('train_loss.pkl', 'wb') as f:\n",
    "        #pickle.dump(master_train_loss, f)\n",
    "    #with open('valid_loss.pkl', 'wb') as f:\n",
    "        #pickle.dump(master_valid_loss, f)\n",
    "        \n",
    "        #torch.save(model.state_dict(), 'model-resnext-50-{}.pth'.format(epoch))\n",
    "        #best_valid_loss = valid_loss\n",
    "    \n",
    "    epoch += 1\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
