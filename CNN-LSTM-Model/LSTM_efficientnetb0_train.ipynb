{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "from tqdm import tqdm as tqdm\n",
    "import cv2\n",
    "import pickle\n",
    "from shutil import copyfile\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Level\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudyLevelDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Kaggle PE dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, stage):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.pedataframe = pd.read_csv(csv_file)\n",
    "        \n",
    "        self.hdf5_filename = '/scratch/features.hdf5'\n",
    "        \n",
    "        self._generate_row_indices_list()\n",
    "        \n",
    "        self.stage = stage\n",
    "        \n",
    "        if self.stage == 'train':\n",
    "            self.start = 0\n",
    "            self.end = 2650\n",
    "        else:\n",
    "            self.start = 2660\n",
    "            self.end = 3500\n",
    "        \n",
    "        # 0 - 2650 train\n",
    "        # 2660 - 3500 valid\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Return number of 2D images. (Each CT slice is an independent image.)\"\"\"\n",
    "        #return len(self.pedataframe)\n",
    "        return self.end - self.start + 1\n",
    "        #return num_pos_samples_per_dataset + num_neg_samples_per_dataset\n",
    "        \n",
    "    def _generate_row_indices_list(self):\n",
    "        # group slice indices into studies\n",
    "        self.row_indices = [] # index into train df for every study\n",
    "        current_study_id = ''\n",
    "\n",
    "        for slice_index in range(len(self.pedataframe)):\n",
    "            new_study_id = self.pedataframe.StudyInstanceUID[slice_index]\n",
    "            if new_study_id != current_study_id:\n",
    "                self.row_indices.append(slice_index)\n",
    "                current_study_id = new_study_id\n",
    "            if slice_index % 100000 == 0:\n",
    "                print(slice_index)\n",
    "        \n",
    "    def __getitem__(self, study_index):\n",
    "        '''  '''\n",
    "        study_index = study_index + self.start\n",
    "        \n",
    "        h5py_file = h5py.File(self.hdf5_filename, \"r\")\n",
    "        \n",
    "        start_index = self.row_indices[study_index]\n",
    "        if study_index+1 < len(self.row_indices):\n",
    "            stop_index = self.row_indices[study_index+1]\n",
    "        else:\n",
    "            stop_index = len(self.pedataframe)\n",
    "            \n",
    "        # dim: seq_len x embedding_dim\n",
    "        x = np.ones((stop_index-start_index, 1280))  #embedding dim = 1280 for efficientnetb0\n",
    "        y = np.ones(stop_index-start_index)\n",
    "            \n",
    "        for slice_index in range(start_index, stop_index):\n",
    "            data_identifier = 'tensor(' + str(slice_index) + ')'\n",
    "            slice_embeddings = h5py_file[data_identifier][:]\n",
    "            x[slice_index-start_index,:] = slice_embeddings\n",
    "            \n",
    "            pe_present_on_image = float(int(self.pedataframe.pe_present_on_image[slice_index]))\n",
    "            y[slice_index-start_index] = pe_present_on_image\n",
    "        \n",
    "        h5py_file.close()\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_filename = '/scratch/features.hdf5'\n",
    "h5py_file = h5py.File(hdf5_filename, \"r\")\n",
    "h5py_file['tensor(12)'][:]\n",
    "\n",
    "h5py_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/projectnb/ece601/kaggle-pulmonary-embolism/rsna-str-pulmonary-embolism-detection/'\n",
    "train_csv = data_dir + 'train.csv'\n",
    "train_dir = data_dir + 'train/'\n",
    "train_dataset = StudyLevelDataset(csv_file=train_csv, stage='train')\n",
    "valid_dataset = StudyLevelDataset(csv_file=train_csv, stage='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e\n",
    "'''\n",
    "\n",
    "batch_size = 1\n",
    "embedding_dim = 1280\n",
    "nb_lstm_units = 64\n",
    "\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, nb_layers=1, nb_lstm_units=nb_lstm_units, \n",
    "                 embedding_dim=embedding_dim, batch_size=batch_size):\n",
    "        \n",
    "        super(MyLSTM, self).__init__()\n",
    "\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # output layer\n",
    "        self.linear = nn.Linear(self.nb_lstm_units, 1)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_h0 = torch.randn(self.nb_layers, self.batch_size, self.nb_lstm_units)\n",
    "        hidden_c0 = torch.randn(self.nb_layers, self.batch_size, self.nb_lstm_units)\n",
    "\n",
    "        hidden_h0 = hidden_h0.to(device)\n",
    "        hidden_c0 = hidden_c0.to(device)\n",
    "\n",
    "        hidden_h0 = Variable(hidden_h0)\n",
    "        hidden_c0 = Variable(hidden_c0)\n",
    "\n",
    "        return (hidden_h0.to(device), hidden_c0.to(device))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch. Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        #X = torch.nn.utils.rnn.pack_padded_sequence(x, X_lengths, batch_first=True)\n",
    "\n",
    "        # now run through LSTM\n",
    "        X, self.hidden = self.lstm(X, self.hidden)\n",
    "\n",
    "        # undo the packing operation\n",
    "        #X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        # this one is a bit tricky as well. First we need to reshape the data so it goes into the linear layer\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "\n",
    "        # run through actual linear layer\n",
    "        X = self.linear(X)\n",
    "\n",
    "        # ---------------------\n",
    "        # 4. Create softmax activations bc we're doing classification\n",
    "        # Dim transformation: (batch_size * seq_len, nb_lstm_units) -> (batch_size, seq_len, nb_tags)\n",
    "        #X = torch.sigmoid(X)\n",
    "\n",
    "        # I like to reshape for mental sanity so we're back to (batch_size, seq_len, nb_tags)\n",
    "        X = X.view(batch_size, seq_len)\n",
    "\n",
    "        Y_hat = X\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLSTM(\n",
       "  (lstm): LSTM(1280, 64, batch_first=True)\n",
       "  (linear): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyLSTM()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model.to(device)\n",
    "x,y = train_dataset[1]\n",
    "x = torch.tensor(x)\n",
    "x = x.unsqueeze(0).float()\n",
    "x = x.to(device)\n",
    "\n",
    "pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 11:04:33 2020 Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.63529, mean: 0.12839: 100%|██████████| 2651/2651 [04:04<00:00, 10.83it/s]\n",
      "100%|██████████| 841/841 [01:19<00:00, 10.61it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 11:09:57 2020 Epoch 1, lr: 0.1000000, train loss: 0.12839, valid loss: 0.12487\n",
      "pos loss: 1.17129, neg loss: 0.06634, pos mean: 0.45012, neg mean 0.05509\n",
      "Tue Dec  8 11:09:57 2020 Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.61882, mean: 0.12375: 100%|██████████| 2651/2651 [04:01<00:00, 10.96it/s]\n",
      "100%|██████████| 841/841 [01:18<00:00, 10.76it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 11:15:17 2020 Epoch 2, lr: 0.1000000, train loss: 0.12375, valid loss: 0.12043\n",
      "pos loss: 1.13419, neg loss: 0.06367, pos mean: 0.46676, neg mean 0.05254\n",
      "Tue Dec  8 11:15:17 2020 Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.61231, mean: 0.12157: 100%|██████████| 2651/2651 [04:02<00:00, 10.93it/s]\n",
      "100%|██████████| 841/841 [01:17<00:00, 10.80it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 11:20:37 2020 Epoch 3, lr: 0.1000000, train loss: 0.12157, valid loss: 0.11103\n",
      "pos loss: 1.23130, neg loss: 0.04848, pos mean: 0.44312, neg mean 0.04011\n",
      "Tue Dec  8 11:20:37 2020 Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.63018, mean: 0.11882: 100%|██████████| 2651/2651 [04:02<00:00, 10.92it/s]\n",
      "100%|██████████| 841/841 [01:17<00:00, 10.88it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 11:25:57 2020 Epoch 4, lr: 0.1000000, train loss: 0.11882, valid loss: 0.10838\n",
      "pos loss: 1.23142, neg loss: 0.04574, pos mean: 0.45890, neg mean 0.03731\n",
      "Tue Dec  8 11:25:57 2020 Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.63074, mean: 0.11813: 100%|██████████| 2651/2651 [04:04<00:00, 10.85it/s]\n",
      "100%|██████████| 841/841 [01:16<00:00, 11.02it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 11:31:18 2020 Epoch 5, lr: 0.1000000, train loss: 0.11813, valid loss: 0.10818\n",
      "pos loss: 1.17500, neg loss: 0.04841, pos mean: 0.48427, neg mean 0.03874\n",
      "Tue Dec  8 11:31:18 2020 Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.62909, mean: 0.11799: 100%|██████████| 2651/2651 [04:02<00:00, 10.93it/s]\n",
      "100%|██████████| 841/841 [01:17<00:00, 10.80it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 11:36:38 2020 Epoch 6, lr: 0.1000000, train loss: 0.11799, valid loss: 0.10800\n",
      "pos loss: 1.17124, neg loss: 0.04854, pos mean: 0.48483, neg mean 0.03887\n",
      "Tue Dec  8 11:36:38 2020 Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.61221, mean: 0.11733: 100%|██████████| 2651/2651 [04:02<00:00, 10.93it/s]\n",
      "100%|██████████| 841/841 [01:16<00:00, 11.00it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 11:41:57 2020 Epoch 7, lr: 0.1000000, train loss: 0.11733, valid loss: 0.10741\n",
      "pos loss: 1.20368, neg loss: 0.04607, pos mean: 0.48312, neg mean 0.03684\n",
      "Tue Dec  8 11:41:57 2020 Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.62918, mean: 0.11703: 100%|██████████| 2651/2651 [04:07<00:00, 10.72it/s]\n",
      "100%|██████████| 841/841 [01:16<00:00, 10.93it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 11:47:22 2020 Epoch 8, lr: 0.1000000, train loss: 0.11703, valid loss: 0.10863\n",
      "pos loss: 1.14463, neg loss: 0.05072, pos mean: 0.50119, neg mean 0.03988\n",
      "Tue Dec  8 11:47:22 2020 Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.61138, mean: 0.11644: 100%|██████████| 2651/2651 [03:59<00:00, 11.06it/s]\n",
      "100%|██████████| 841/841 [01:15<00:00, 11.20it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 11:52:37 2020 Epoch 9, lr: 0.1000000, train loss: 0.11644, valid loss: 0.10624\n",
      "pos loss: 1.15733, neg loss: 0.04738, pos mean: 0.49632, neg mean 0.03761\n",
      "Tue Dec  8 11:52:37 2020 Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.62202, mean: 0.11652: 100%|██████████| 2651/2651 [03:51<00:00, 11.43it/s]\n",
      "100%|██████████| 841/841 [01:15<00:00, 11.16it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 11:57:44 2020 Epoch 10, lr: 0.1000000, train loss: 0.11652, valid loss: 0.10738\n",
      "pos loss: 1.18630, neg loss: 0.04699, pos mean: 0.48915, neg mean 0.03742\n",
      "Tue Dec  8 11:57:44 2020 Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.65022, mean: 0.11666: 100%|██████████| 2651/2651 [03:49<00:00, 11.57it/s]\n",
      "100%|██████████| 841/841 [01:15<00:00, 11.19it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 12:02:48 2020 Epoch 11, lr: 0.1000000, train loss: 0.11666, valid loss: 0.10761\n",
      "pos loss: 1.13905, neg loss: 0.04980, pos mean: 0.49909, neg mean 0.03957\n",
      "Tue Dec  8 12:02:48 2020 Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.61003, mean: 0.11656: 100%|██████████| 2651/2651 [03:58<00:00, 11.12it/s]\n",
      "100%|██████████| 841/841 [01:19<00:00, 10.61it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 12:08:06 2020 Epoch 12, lr: 0.1000000, train loss: 0.11656, valid loss: 0.10702\n",
      "pos loss: 1.12150, neg loss: 0.05022, pos mean: 0.50476, neg mean 0.03983\n",
      "Tue Dec  8 12:08:06 2020 Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.67068, mean: 0.11619: 100%|██████████| 2651/2651 [03:49<00:00, 11.54it/s]\n",
      "100%|██████████| 841/841 [01:19<00:00, 10.59it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 12:13:15 2020 Epoch 13, lr: 0.1000000, train loss: 0.11619, valid loss: 0.10627\n",
      "pos loss: 1.21742, neg loss: 0.04414, pos mean: 0.48315, neg mean 0.03524\n",
      "Tue Dec  8 12:13:15 2020 Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.62177, mean: 0.11636: 100%|██████████| 2651/2651 [03:51<00:00, 11.43it/s]\n",
      "100%|██████████| 841/841 [01:14<00:00, 11.26it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 12:18:22 2020 Epoch 14, lr: 0.1000000, train loss: 0.11636, valid loss: 0.10580\n",
      "pos loss: 1.19390, neg loss: 0.04494, pos mean: 0.47953, neg mean 0.03641\n",
      "Tue Dec  8 12:18:22 2020 Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.64333, mean: 0.11623: 100%|██████████| 2651/2651 [03:49<00:00, 11.55it/s]\n",
      "100%|██████████| 841/841 [01:17<00:00, 10.80it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 12:23:29 2020 Epoch 15, lr: 0.1000000, train loss: 0.11623, valid loss: 0.10672\n",
      "pos loss: 1.14549, neg loss: 0.04849, pos mean: 0.50019, neg mean 0.03846\n",
      "Tue Dec  8 12:23:29 2020 Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.61656, mean: 0.11629: 100%|██████████| 2651/2651 [03:46<00:00, 11.70it/s]\n",
      "100%|██████████| 841/841 [01:19<00:00, 10.56it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 12:28:35 2020 Epoch 16, lr: 0.1000000, train loss: 0.11629, valid loss: 0.10621\n",
      "pos loss: 1.19819, neg loss: 0.04518, pos mean: 0.49082, neg mean 0.03578\n",
      "Tue Dec  8 12:28:35 2020 Epoch: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.62907, mean: 0.11614: 100%|██████████| 2651/2651 [04:04<00:00, 10.84it/s]\n",
      "100%|██████████| 841/841 [01:15<00:00, 11.14it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 12:33:55 2020 Epoch 17, lr: 0.1000000, train loss: 0.11614, valid loss: 0.10637\n",
      "pos loss: 1.16981, neg loss: 0.04693, pos mean: 0.49509, neg mean 0.03723\n",
      "Tue Dec  8 12:33:55 2020 Epoch: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.65576, mean: 0.11644: 100%|██████████| 2651/2651 [03:52<00:00, 11.42it/s]\n",
      "100%|██████████| 841/841 [01:18<00:00, 10.67it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 12:39:06 2020 Epoch 18, lr: 0.1000000, train loss: 0.11644, valid loss: 0.10698\n",
      "pos loss: 1.13271, neg loss: 0.04956, pos mean: 0.50517, neg mean 0.03911\n",
      "Tue Dec  8 12:39:06 2020 Epoch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.65164, mean: 0.11611: 100%|██████████| 2651/2651 [03:55<00:00, 11.28it/s]\n",
      "  0%|          | 0/841 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x2ac3bf0e5cb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg.7/pytorch/1.5.1/install/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/share/pkg.7/pytorch/1.5.1/install/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/share/pkg.7/python3/3.7.7/install/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "100%|██████████| 841/841 [01:15<00:00, 11.09it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 12:44:17 2020 Epoch 19, lr: 0.1000000, train loss: 0.11611, valid loss: 0.10584\n",
      "pos loss: 1.17782, neg loss: 0.04578, pos mean: 0.49396, neg mean 0.03631\n",
      "Tue Dec  8 12:44:17 2020 Epoch: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x2ac3bf0e5cb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg.7/pytorch/1.5.1/install/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/share/pkg.7/pytorch/1.5.1/install/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/share/pkg.7/python3/3.7.7/install/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "loss: 0.72533, mean: 0.72533:   0%|          | 1/2651 [00:00<12:30,  3.53it/s]assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "loss: 0.62427, mean: 0.11631: 100%|██████████| 2651/2651 [03:54<00:00, 11.30it/s]\n",
      "100%|██████████| 841/841 [01:15<00:00, 11.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 12:49:27 2020 Epoch 20, lr: 0.1000000, train loss: 0.11631, valid loss: 0.10554\n",
      "pos loss: 1.20415, neg loss: 0.04421, pos mean: 0.48517, neg mean 0.03541\n"
     ]
    }
   ],
   "source": [
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "def criterion(logits, target):\n",
    "    loss = bce(logits.view(-1), target.view(-1))\n",
    "    return loss\n",
    "\n",
    "dummy = None\n",
    "\n",
    "def train_epoch(model, loader, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader)\n",
    "    for (data, target) in bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward pass\n",
    "        logits = model(data.float())\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        # backpropagate the loss (backward pass)\n",
    "        loss.backward()\n",
    " \n",
    "        # update parameters based on accumulated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "\n",
    "        train_loss.append(loss_np)\n",
    "        average_loss = sum(train_loss) / len(train_loss)\n",
    "        bar.set_description('loss: %.5f, mean: %.5f' % (loss_np, average_loss))\n",
    "\n",
    "    return float(average_loss)\n",
    "\n",
    "def valid_epoch(model, loader):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    pos_logits = []\n",
    "    neg_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in tqdm(loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits = model(data.float())\n",
    "\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            loss_np = loss.detach().cpu().numpy()\n",
    "\n",
    "            val_loss.append(float(loss_np))\n",
    "\n",
    "            logits = logits.squeeze()\n",
    "            target = target.squeeze()\n",
    "            \n",
    "            for i in range(logits.shape[-1]):\n",
    "                b = target[i].detach().cpu().numpy()\n",
    "                if b == 1:\n",
    "                    pos_logits.append(float(logits[i].detach().cpu().numpy()))\n",
    "                else:\n",
    "                    neg_logits.append(float(logits[i].detach().cpu().numpy()))\n",
    "                    \n",
    "    val_loss_mean = sum(val_loss) / len(val_loss)\n",
    "    \n",
    "    neg_logits_tensor = torch.FloatTensor(neg_logits).cuda()\n",
    "    pos_logits_tensor = torch.FloatTensor(pos_logits).cuda()\n",
    "    \n",
    "    neg_loss = criterion(neg_logits_tensor, torch.zeros(neg_logits_tensor.shape).float().cuda())\n",
    "    pos_loss = criterion(pos_logits_tensor, torch.ones(pos_logits_tensor.shape).float().cuda())\n",
    "    \n",
    "    neg_loss = neg_loss.detach().cpu().numpy()\n",
    "    pos_loss = pos_loss.detach().cpu().numpy()\n",
    "    \n",
    "    neg_mean = float(torch.sigmoid(neg_logits_tensor).mean().detach().cpu().numpy())\n",
    "    pos_mean = float(torch.sigmoid(pos_logits_tensor).mean().detach().cpu().numpy())\n",
    "    \n",
    "    return float(val_loss_mean), float(pos_loss), float(neg_loss), pos_mean, neg_mean\n",
    "\n",
    "def get_optimizer(lr, model):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0001)\n",
    "    return optimizer\n",
    "\n",
    "init_lr = 0.1\n",
    "n_epochs = 20\n",
    "device = 'cuda'\n",
    "\n",
    "# reduce LR by gamma three times every 10 epochs\n",
    "# each epoch is 100,000 samples\n",
    "#gamma = 10\n",
    "#schedule = [10, 20, 30]\n",
    "\n",
    "#model = resnext101.to(device)\n",
    "optimizer = get_optimizer(init_lr, model)\n",
    "model.to(device)\n",
    "\n",
    "master_train_loss = []\n",
    "master_valid_loss = []\n",
    "epoch = 1\n",
    "#best_valid_loss = 10\n",
    "\n",
    "\n",
    "while epoch <= n_epochs:\n",
    "    print(time.ctime(), 'Epoch:', epoch)\n",
    "    \n",
    "    # update learning rate \n",
    "    #if epoch in schedule:\n",
    "    #    new_lr = optimizer.param_groups[0][\"lr\"] / gamma\n",
    "     #   optimizer = get_optimizer(new_lr, model)\n",
    "    \n",
    "    #train_loader, valid_loader = get_loaders(epoch)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1,num_workers=1)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1,num_workers=1)\n",
    "\n",
    "    # train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # validate\n",
    "    valid_loss, pos_loss, neg_loss, pos_mean, neg_mean = valid_epoch(model, valid_loader)\n",
    "    #valid_loss = 0\n",
    "    #pos_loss  = 0\n",
    "    #neg_loss = 0\n",
    "    #pos_mean = 0\n",
    "    #neg_mean = 0\n",
    "    \n",
    "    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, valid loss: {(valid_loss):.5f}'\n",
    "    print(content)\n",
    "    content = f'pos loss: {(pos_loss):.5f}, neg loss: {(neg_loss):.5f}, pos mean: {(pos_mean):.5f}, neg mean {(neg_mean):.5f}'\n",
    "    print(content)\n",
    "    master_train_loss.append(train_loss)\n",
    "    master_valid_loss.append(valid_loss)\n",
    "    \n",
    "    # save loss data and model weights\n",
    "    #with open('train_loss.pkl', 'wb') as f:\n",
    "        #pickle.dump(master_train_loss, f)\n",
    "    #with open('valid_loss.pkl', 'wb') as f:\n",
    "        #pickle.dump(master_valid_loss, f)\n",
    "        \n",
    "        #torch.save(model.state_dict(), 'model-resnext-50-{}.pth'.format(epoch))\n",
    "        #best_valid_loss = valid_loss\n",
    "    \n",
    "    epoch += 1\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ac48e858110>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'model-efficientb0-lstm1.pth')\n",
    "plt.plot(master_train_loss[:], label='train')\n",
    "plt.plot(master_valid_loss[:], label='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 15:14:10 2020 Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.63551, mean: 0.12826: 100%|██████████| 2651/2651 [04:02<00:00, 10.94it/s]\n",
      "100%|██████████| 841/841 [01:20<00:00, 10.38it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 15:19:33 2020 Epoch 1, lr: 0.1000000, train loss: 0.12826, valid loss: 0.12449\n",
      "pos loss: 1.16557, neg loss: 0.06627, pos mean: 0.45283, neg mean 0.05498\n",
      "Tue Dec  8 15:19:33 2020 Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.58394, mean: 0.12234: 100%|██████████| 2651/2651 [03:54<00:00, 11.28it/s]\n",
      "100%|██████████| 841/841 [01:16<00:00, 10.99it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 15:24:45 2020 Epoch 2, lr: 0.1000000, train loss: 0.12234, valid loss: 0.11475\n",
      "pos loss: 1.23261, neg loss: 0.05226, pos mean: 0.42898, neg mean 0.04396\n",
      "Tue Dec  8 15:24:45 2020 Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.00883, mean: 0.13749:   2%|▏         | 54/2651 [00:03<02:43, 15.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-cc41bcc124c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-cc41bcc124c4>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.7/install/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.5.1/install/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.5.1/install/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.5.1/install/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.5.1/install/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.7/install/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.5.1/install/3.7/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    292\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.7/install/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.7/install/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.7/install/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauthkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0manswer_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         \u001b[0mdeliver_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.7/install/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    739\u001b[0m         raise ValueError(\n\u001b[1;32m    740\u001b[0m             \"Authkey must be bytes, not {0!s}\".format(type(authkey)))\n\u001b[0;32m--> 741\u001b[0;31m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# reject large message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCHALLENGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message = %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.7/install/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.7/install/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.7/install/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "def criterion(logits, target):\n",
    "    loss = bce(logits.view(-1), target.view(-1))\n",
    "    return loss\n",
    "\n",
    "dummy = None\n",
    "\n",
    "def train_epoch(model, loader, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader)\n",
    "    for (data, target) in bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward pass\n",
    "        logits = model(data.float())\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        # backpropagate the loss (backward pass)\n",
    "        loss.backward()\n",
    " \n",
    "        # update parameters based on accumulated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "\n",
    "        train_loss.append(loss_np)\n",
    "        average_loss = sum(train_loss) / len(train_loss)\n",
    "        bar.set_description('loss: %.5f, mean: %.5f' % (loss_np, average_loss))\n",
    "\n",
    "    return float(average_loss)\n",
    "\n",
    "def valid_epoch(model, loader):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    pos_logits = []\n",
    "    neg_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in tqdm(loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits = model(data.float())\n",
    "\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            loss_np = loss.detach().cpu().numpy()\n",
    "\n",
    "            val_loss.append(float(loss_np))\n",
    "\n",
    "            logits = logits.squeeze()\n",
    "            target = target.squeeze()\n",
    "            \n",
    "            for i in range(logits.shape[-1]):\n",
    "                b = target[i].detach().cpu().numpy()\n",
    "                if b == 1:\n",
    "                    pos_logits.append(float(logits[i].detach().cpu().numpy()))\n",
    "                else:\n",
    "                    neg_logits.append(float(logits[i].detach().cpu().numpy()))\n",
    "                    \n",
    "    val_loss_mean = sum(val_loss) / len(val_loss)\n",
    "    \n",
    "    neg_logits_tensor = torch.FloatTensor(neg_logits).cuda()\n",
    "    pos_logits_tensor = torch.FloatTensor(pos_logits).cuda()\n",
    "    \n",
    "    neg_loss = criterion(neg_logits_tensor, torch.zeros(neg_logits_tensor.shape).float().cuda())\n",
    "    pos_loss = criterion(pos_logits_tensor, torch.ones(pos_logits_tensor.shape).float().cuda())\n",
    "    \n",
    "    neg_loss = neg_loss.detach().cpu().numpy()\n",
    "    pos_loss = pos_loss.detach().cpu().numpy()\n",
    "    \n",
    "    neg_mean = float(torch.sigmoid(neg_logits_tensor).mean().detach().cpu().numpy())\n",
    "    pos_mean = float(torch.sigmoid(pos_logits_tensor).mean().detach().cpu().numpy())\n",
    "    \n",
    "    return float(val_loss_mean), float(pos_loss), float(neg_loss), pos_mean, neg_mean\n",
    "\n",
    "def get_optimizer(lr, model):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0001)\n",
    "    return optimizer\n",
    "\n",
    "init_lr = 0.1\n",
    "n_epochs = 5\n",
    "device = 'cuda'\n",
    "\n",
    "# reduce LR by gamma three times every 10 epochs\n",
    "# each epoch is 100,000 samples\n",
    "#gamma = 10\n",
    "#schedule = [10, 20, 30]\n",
    "\n",
    "#model = resnext101.to(device)\n",
    "optimizer = get_optimizer(init_lr, model)\n",
    "model.to(device)\n",
    "\n",
    "master_train_loss = []\n",
    "master_valid_loss = []\n",
    "epoch = 1\n",
    "#best_valid_loss = 10\n",
    "\n",
    "\n",
    "while epoch <= n_epochs:\n",
    "    print(time.ctime(), 'Epoch:', epoch)\n",
    "    \n",
    "    # update learning rate \n",
    "    #if epoch in schedule:\n",
    "    #    new_lr = optimizer.param_groups[0][\"lr\"] / gamma\n",
    "     #   optimizer = get_optimizer(new_lr, model)\n",
    "    \n",
    "    #train_loader, valid_loader = get_loaders(epoch)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1,num_workers=1)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1,num_workers=1)\n",
    "\n",
    "    # train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # validate\n",
    "    valid_loss, pos_loss, neg_loss, pos_mean, neg_mean = valid_epoch(model, valid_loader)\n",
    "    #valid_loss = 0\n",
    "    #pos_loss  = 0\n",
    "    #neg_loss = 0\n",
    "    #pos_mean = 0\n",
    "    #neg_mean = 0\n",
    "    \n",
    "    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, valid loss: {(valid_loss):.5f}'\n",
    "    print(content)\n",
    "    content = f'pos loss: {(pos_loss):.5f}, neg loss: {(neg_loss):.5f}, pos mean: {(pos_mean):.5f}, neg mean {(neg_mean):.5f}'\n",
    "    print(content)\n",
    "    master_train_loss.append(train_loss)\n",
    "    master_valid_loss.append(valid_loss)\n",
    "    \n",
    "    # save loss data and model weights\n",
    "    #with open('train_loss.pkl', 'wb') as f:\n",
    "        #pickle.dump(master_train_loss, f)\n",
    "    #with open('valid_loss.pkl', 'wb') as f:\n",
    "        #pickle.dump(master_valid_loss, f)\n",
    "        \n",
    "        #torch.save(model.state_dict(), 'model-resnext-50-{}.pth'.format(epoch))\n",
    "        #best_valid_loss = valid_loss\n",
    "    \n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model-efficientb0-lstm1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study Level Metrics\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudyLevelDataset2(torch.utils.data.Dataset):\n",
    "    \"\"\"Kaggle PE dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, stage):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.pedataframe = pd.read_csv(csv_file)\n",
    "        \n",
    "        self.hdf5_filename = '/scratch/features.hdf5'\n",
    "        \n",
    "        self._generate_row_indices_list()\n",
    "        \n",
    "        self.stage = stage\n",
    "        \n",
    "        if self.stage == 'train':\n",
    "            self.start = 0\n",
    "            self.end = 2650\n",
    "        else:\n",
    "            self.start = 2660\n",
    "            self.end = 3500\n",
    "        \n",
    "        # 0 - 2650 train\n",
    "        # 2660 - 3500 valid\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Return number of 2D images. (Each CT slice is an independent image.)\"\"\"\n",
    "        #return len(self.pedataframe)\n",
    "        return self.end - self.start + 1\n",
    "        #return num_pos_samples_per_dataset + num_neg_samples_per_dataset\n",
    "        \n",
    "    def _generate_row_indices_list(self):\n",
    "        # group slice indices into studies\n",
    "        self.row_indices = [] # index into train df for every study\n",
    "        current_study_id = ''\n",
    "\n",
    "        for slice_index in range(len(self.pedataframe)):\n",
    "            new_study_id = self.pedataframe.StudyInstanceUID[slice_index]\n",
    "            if new_study_id != current_study_id:\n",
    "                self.row_indices.append(slice_index)\n",
    "                current_study_id = new_study_id\n",
    "            if slice_index % 100000 == 0:\n",
    "                print(slice_index)\n",
    "        \n",
    "    def __getitem__(self, study_index):\n",
    "        '''  '''\n",
    "        study_index = study_index + self.start\n",
    "        \n",
    "        h5py_file = h5py.File(self.hdf5_filename, \"r\")\n",
    "        \n",
    "        start_index = self.row_indices[study_index]\n",
    "        if study_index+1 < len(self.row_indices):\n",
    "            stop_index = self.row_indices[study_index+1]\n",
    "        else:\n",
    "            stop_index = len(self.pedataframe)\n",
    "            \n",
    "        # dim: seq_len x embedding_dim\n",
    "        x = np.ones((stop_index-start_index, 1280))\n",
    "            \n",
    "        for slice_index in range(start_index, stop_index):\n",
    "            data_identifier = 'tensor(' + str(slice_index) + ')'\n",
    "            slice_embeddings = h5py_file[data_identifier][:]\n",
    "            x[slice_index-start_index,:] = slice_embeddings\n",
    "\n",
    "            \n",
    "        y = np.ones(9)\n",
    "            \n",
    "        y[0] =  self.pedataframe.negative_exam_for_pe[start_index]\n",
    "        y[1] = self.pedataframe.rv_lv_ratio_gte_1[start_index]\n",
    "        y[2] = self.pedataframe.rv_lv_ratio_lt_1[start_index]\n",
    "        y[3] = self.pedataframe.leftsided_pe[start_index]\n",
    "        y[4] = self.pedataframe.chronic_pe[start_index]\n",
    "        y[5] = self.pedataframe.rightsided_pe[start_index]\n",
    "        y[6] = self.pedataframe.acute_and_chronic_pe[start_index]\n",
    "        y[7] = self.pedataframe.central_pe[start_index]\n",
    "        y[8] = self.pedataframe.indeterminate[start_index]\n",
    "        \n",
    "        h5py_file.close()\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/projectnb/ece601/kaggle-pulmonary-embolism/rsna-str-pulmonary-embolism-detection/'\n",
    "train_csv = data_dir + 'train.csv'\n",
    "train_dir = data_dir + 'train/'\n",
    "train_dataset = StudyLevelDataset2(csv_file=train_csv, stage='train')\n",
    "valid_dataset = StudyLevelDataset2(csv_file=train_csv, stage='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e\n",
    "'''\n",
    "\n",
    "batch_size = 1\n",
    "embedding_dim = 1280  #2048\n",
    "nb_lstm_units = 64 #2048\n",
    "\n",
    "class StudyLevelLSTM(nn.Module):\n",
    "    def __init__(self, nb_layers=1, nb_lstm_units=nb_lstm_units, \n",
    "                 embedding_dim=embedding_dim, batch_size=batch_size):\n",
    "        \n",
    "        super(StudyLevelLSTM, self).__init__()\n",
    "\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # output layer\n",
    "        self.linear = nn.Linear(self.nb_lstm_units, 9)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_h0 = torch.randn(self.nb_layers, self.batch_size, self.nb_lstm_units)\n",
    "        hidden_c0 = torch.randn(self.nb_layers, self.batch_size, self.nb_lstm_units)\n",
    "\n",
    "        hidden_h0 = hidden_h0.to(device)\n",
    "        hidden_c0 = hidden_c0.to(device)\n",
    "\n",
    "        hidden_h0 = Variable(hidden_h0)\n",
    "        hidden_c0 = Variable(hidden_c0)\n",
    "\n",
    "        return (hidden_h0.to(device), hidden_c0.to(device))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch. Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        #X = torch.nn.utils.rnn.pack_padded_sequence(x, X_lengths, batch_first=True)\n",
    "\n",
    "        # now run through LSTM\n",
    "        X, self.hidden = self.lstm(X, self.hidden)\n",
    "        \n",
    "        X = self.hidden[0]\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "\n",
    "        # run through actual linear layer\n",
    "        X = self.linear(X)\n",
    "\n",
    "        # I like to reshape for mental sanity so we're back to (batch_size, seq_len, nb_tags)\n",
    "        X = X.view(batch_size, 9)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StudyLevelLSTM(\n",
      "  (lstm): LSTM(1280, 64, batch_first=True)\n",
      "  (linear): Linear(in_features=64, out_features=9, bias=True)\n",
      ")\n",
      "tensor([[-0.0184, -0.0487,  0.0485,  0.0937,  0.0497,  0.0606,  0.1190,  0.1331,\n",
      "          0.0840]], grad_fn=<ViewBackward>)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# sanity test\n",
    "\n",
    "model = StudyLevelLSTM()\n",
    "print(model)\n",
    "device = 'cpu'\n",
    "model.to(device)\n",
    "x,y = train_dataset[1]\n",
    "x = torch.tensor(x)\n",
    "x = x.unsqueeze(0).float()\n",
    "x = x.to(device)\n",
    "\n",
    "pred = model(x)\n",
    "print(pred)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 13:04:03 2020 Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.87645, mean: 0.32227: 100%|██████████| 2651/2651 [03:16<00:00, 13.48it/s]\n",
      "100%|██████████| 841/841 [00:57<00:00, 14.50it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 13:08:18 2020 Epoch 1, lr: 0.1000000, train loss: 0.32227, valid loss: 0.29737\n",
      "pos loss: 0.90480, neg loss: 0.17502, pos mean: 0.48567, neg mean 0.14328\n",
      "Tue Dec  8 13:08:18 2020 Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.99887, mean: 0.29031: 100%|██████████| 2651/2651 [03:16<00:00, 13.50it/s]\n",
      "100%|██████████| 841/841 [00:58<00:00, 14.36it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 13:12:33 2020 Epoch 2, lr: 0.1000000, train loss: 0.29031, valid loss: 0.26129\n",
      "pos loss: 0.85286, neg loss: 0.14212, pos mean: 0.54595, neg mean 0.11308\n",
      "Tue Dec  8 13:12:33 2020 Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.98081, mean: 0.28251: 100%|██████████| 2651/2651 [03:16<00:00, 13.51it/s]\n",
      "100%|██████████| 841/841 [00:59<00:00, 14.04it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 13:16:49 2020 Epoch 3, lr: 0.1000000, train loss: 0.28251, valid loss: 0.27183\n",
      "pos loss: 0.87397, neg loss: 0.15054, pos mean: 0.51461, neg mean 0.12310\n",
      "Tue Dec  8 13:16:49 2020 Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.04946, mean: 0.28119: 100%|██████████| 2651/2651 [03:13<00:00, 13.71it/s]\n",
      "100%|██████████| 841/841 [01:03<00:00, 13.19it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 13:21:06 2020 Epoch 4, lr: 0.1000000, train loss: 0.28119, valid loss: 0.26057\n",
      "pos loss: 0.85670, neg loss: 0.14049, pos mean: 0.54804, neg mean 0.11115\n",
      "Tue Dec  8 13:21:06 2020 Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.97003, mean: 0.28370: 100%|██████████| 2651/2651 [03:11<00:00, 13.87it/s]\n",
      "100%|██████████| 841/841 [00:59<00:00, 14.06it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 13:25:17 2020 Epoch 5, lr: 0.1000000, train loss: 0.28370, valid loss: 0.26230\n",
      "pos loss: 0.83399, neg loss: 0.14714, pos mean: 0.56013, neg mean 0.11480\n",
      "Tue Dec  8 13:25:17 2020 Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.03984, mean: 0.28227: 100%|██████████| 2651/2651 [03:14<00:00, 13.61it/s]\n",
      "100%|██████████| 841/841 [01:00<00:00, 13.99it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 13:29:32 2020 Epoch 6, lr: 0.1000000, train loss: 0.28227, valid loss: 0.25548\n",
      "pos loss: 0.85803, neg loss: 0.13411, pos mean: 0.55102, neg mean 0.10682\n",
      "Tue Dec  8 13:29:32 2020 Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.02507, mean: 0.27940: 100%|██████████| 2651/2651 [03:07<00:00, 14.14it/s]\n",
      "100%|██████████| 841/841 [00:59<00:00, 14.06it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 13:33:39 2020 Epoch 7, lr: 0.1000000, train loss: 0.27940, valid loss: 0.25660\n",
      "pos loss: 0.84777, neg loss: 0.13752, pos mean: 0.55912, neg mean 0.10806\n",
      "Tue Dec  8 13:33:39 2020 Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.02191, mean: 0.27916: 100%|██████████| 2651/2651 [03:11<00:00, 13.87it/s]\n",
      "100%|██████████| 841/841 [01:04<00:00, 13.10it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 13:37:54 2020 Epoch 8, lr: 0.1000000, train loss: 0.27916, valid loss: 0.25679\n",
      "pos loss: 0.84593, neg loss: 0.13812, pos mean: 0.55609, neg mean 0.10930\n",
      "Tue Dec  8 13:37:54 2020 Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.06097, mean: 0.27844: 100%|██████████| 2651/2651 [03:08<00:00, 14.03it/s]\n",
      "100%|██████████| 841/841 [00:59<00:00, 14.22it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 13:42:03 2020 Epoch 9, lr: 0.1000000, train loss: 0.27844, valid loss: 0.25660\n",
      "pos loss: 0.85290, neg loss: 0.13649, pos mean: 0.55578, neg mean 0.10779\n",
      "Tue Dec  8 13:42:03 2020 Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.12612, mean: 0.28120: 100%|██████████| 2651/2651 [03:08<00:00, 14.03it/s]\n",
      "100%|██████████| 841/841 [00:59<00:00, 14.23it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 13:46:11 2020 Epoch 10, lr: 0.1000000, train loss: 0.28120, valid loss: 0.25593\n",
      "pos loss: 0.87095, neg loss: 0.13205, pos mean: 0.54643, neg mean 0.10586\n",
      "Tue Dec  8 13:46:11 2020 Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.94585, mean: 0.28033: 100%|██████████| 2651/2651 [03:10<00:00, 13.92it/s]\n",
      "100%|██████████| 841/841 [00:58<00:00, 14.48it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 13:50:19 2020 Epoch 11, lr: 0.1000000, train loss: 0.28033, valid loss: 0.26006\n",
      "pos loss: 0.83385, neg loss: 0.14449, pos mean: 0.55175, neg mean 0.11505\n",
      "Tue Dec  8 13:50:19 2020 Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.97300, mean: 0.28053: 100%|██████████| 2651/2651 [03:11<00:00, 13.81it/s]\n",
      "100%|██████████| 841/841 [01:00<00:00, 13.95it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 13:54:31 2020 Epoch 12, lr: 0.1000000, train loss: 0.28053, valid loss: 0.26055\n",
      "pos loss: 0.84063, neg loss: 0.14370, pos mean: 0.55221, neg mean 0.11425\n",
      "Tue Dec  8 13:54:31 2020 Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.01264, mean: 0.28134: 100%|██████████| 2651/2651 [03:10<00:00, 13.94it/s]\n",
      "100%|██████████| 841/841 [00:58<00:00, 14.39it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 13:58:40 2020 Epoch 13, lr: 0.1000000, train loss: 0.28134, valid loss: 0.26005\n",
      "pos loss: 0.84268, neg loss: 0.14269, pos mean: 0.54780, neg mean 0.11451\n",
      "Tue Dec  8 13:58:40 2020 Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.00640, mean: 0.28078: 100%|██████████| 2651/2651 [03:08<00:00, 14.08it/s]\n",
      "100%|██████████| 841/841 [01:02<00:00, 13.40it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 14:02:51 2020 Epoch 14, lr: 0.1000000, train loss: 0.28078, valid loss: 0.25934\n",
      "pos loss: 0.83299, neg loss: 0.14379, pos mean: 0.55694, neg mean 0.11419\n",
      "Tue Dec  8 14:02:51 2020 Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.96305, mean: 0.28353: 100%|██████████| 2651/2651 [03:09<00:00, 14.01it/s]\n",
      "100%|██████████| 841/841 [00:59<00:00, 14.25it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 14:06:59 2020 Epoch 15, lr: 0.1000000, train loss: 0.28353, valid loss: 0.26049\n",
      "pos loss: 0.84441, neg loss: 0.14287, pos mean: 0.54822, neg mean 0.11407\n",
      "Tue Dec  8 14:06:59 2020 Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.87608, mean: 0.28085: 100%|██████████| 2651/2651 [03:07<00:00, 14.16it/s]\n",
      "100%|██████████| 841/841 [00:58<00:00, 14.27it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 14:11:05 2020 Epoch 16, lr: 0.1000000, train loss: 0.28085, valid loss: 0.26363\n",
      "pos loss: 0.82732, neg loss: 0.15009, pos mean: 0.56184, neg mean 0.11727\n",
      "Tue Dec  8 14:11:05 2020 Epoch: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.99725, mean: 0.28056: 100%|██████████| 2651/2651 [03:14<00:00, 13.64it/s]\n",
      "100%|██████████| 841/841 [00:59<00:00, 14.04it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 14:15:20 2020 Epoch 17, lr: 0.1000000, train loss: 0.28056, valid loss: 0.25712\n",
      "pos loss: 0.84520, neg loss: 0.13867, pos mean: 0.55605, neg mean 0.10972\n",
      "Tue Dec  8 14:15:20 2020 Epoch: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.97763, mean: 0.27845: 100%|██████████| 2651/2651 [03:08<00:00, 14.05it/s]\n",
      "100%|██████████| 841/841 [01:05<00:00, 12.89it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 14:19:34 2020 Epoch 18, lr: 0.1000000, train loss: 0.27845, valid loss: 0.25745\n",
      "pos loss: 0.84248, neg loss: 0.13961, pos mean: 0.55684, neg mean 0.11026\n",
      "Tue Dec  8 14:19:34 2020 Epoch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.05888, mean: 0.27807: 100%|██████████| 2651/2651 [03:11<00:00, 13.88it/s]\n",
      "100%|██████████| 841/841 [00:57<00:00, 14.50it/s]\n",
      "  0%|          | 0/2651 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 14:23:43 2020 Epoch 19, lr: 0.1000000, train loss: 0.27807, valid loss: 0.25903\n",
      "pos loss: 0.84260, neg loss: 0.14148, pos mean: 0.55899, neg mean 0.11131\n",
      "Tue Dec  8 14:23:43 2020 Epoch: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.97862, mean: 0.28117: 100%|██████████| 2651/2651 [03:10<00:00, 13.91it/s]\n",
      "100%|██████████| 841/841 [00:59<00:00, 14.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 14:27:53 2020 Epoch 20, lr: 0.1000000, train loss: 0.28117, valid loss: 0.25424\n",
      "pos loss: 0.85160, neg loss: 0.13391, pos mean: 0.56291, neg mean 0.10465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "def criterion(logits, target):\n",
    "    loss = bce(logits.view(-1), target.view(-1))\n",
    "    return loss\n",
    "\n",
    "def train_epoch(model, loader, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader)\n",
    "    for (data, target) in bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward pass\n",
    "        logits = model(data.float())\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        # backpropagate the loss (backward pass)\n",
    "        loss.backward()\n",
    " \n",
    "        # update parameters based on accumulated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "\n",
    "        train_loss.append(loss_np)\n",
    "        average_loss = sum(train_loss) / len(train_loss)\n",
    "        bar.set_description('loss: %.5f, mean: %.5f' % (loss_np, average_loss))\n",
    "\n",
    "    return float(average_loss)\n",
    "\n",
    "def valid_epoch(model, loader):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    pos_logits = []\n",
    "    neg_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in tqdm(loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits = model(data.float())\n",
    "\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            loss_np = loss.detach().cpu().numpy()\n",
    "\n",
    "            val_loss.append(float(loss_np))\n",
    "\n",
    "            logits = logits.squeeze()\n",
    "            target = target.squeeze()\n",
    "            \n",
    "            for i in range(logits.shape[-1]):\n",
    "                b = target[i].detach().cpu().numpy()\n",
    "                if b == 1:\n",
    "                    pos_logits.append(float(logits[i].detach().cpu().numpy()))\n",
    "                else:\n",
    "                    neg_logits.append(float(logits[i].detach().cpu().numpy()))\n",
    "                    \n",
    "    val_loss_mean = sum(val_loss) / len(val_loss)\n",
    "    \n",
    "    neg_logits_tensor = torch.FloatTensor(neg_logits).cuda()\n",
    "    pos_logits_tensor = torch.FloatTensor(pos_logits).cuda()\n",
    "    \n",
    "    neg_loss = criterion(neg_logits_tensor, torch.zeros(neg_logits_tensor.shape).float().cuda())\n",
    "    pos_loss = criterion(pos_logits_tensor, torch.ones(pos_logits_tensor.shape).float().cuda())\n",
    "    \n",
    "    neg_loss = neg_loss.detach().cpu().numpy()\n",
    "    pos_loss = pos_loss.detach().cpu().numpy()\n",
    "    \n",
    "    neg_mean = float(torch.sigmoid(neg_logits_tensor).mean().detach().cpu().numpy())\n",
    "    pos_mean = float(torch.sigmoid(pos_logits_tensor).mean().detach().cpu().numpy())\n",
    "    \n",
    "    return float(val_loss_mean), float(pos_loss), float(neg_loss), pos_mean, neg_mean\n",
    "\n",
    "def get_optimizer(lr, model):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0001)\n",
    "    return optimizer\n",
    "\n",
    "init_lr = 0.1\n",
    "n_epochs = 20\n",
    "device = 'cuda'\n",
    "\n",
    "# reduce LR by gamma three times every 10 epochs\n",
    "# each epoch is 100,000 samples\n",
    "#gamma = 10\n",
    "#schedule = [10, 20, 30]\n",
    "\n",
    "#model = resnext101.to(device)\n",
    "optimizer = get_optimizer(init_lr, model)\n",
    "model.to(device)\n",
    "\n",
    "master_train_loss = []\n",
    "master_valid_loss = []\n",
    "epoch = 1\n",
    "#best_valid_loss = 10\n",
    "\n",
    "\n",
    "while epoch <= n_epochs:\n",
    "    print(time.ctime(), 'Epoch:', epoch)\n",
    "    \n",
    "    # update learning rate \n",
    "    #if epoch in schedule:\n",
    "    #    new_lr = optimizer.param_groups[0][\"lr\"] / gamma\n",
    "     #   optimizer = get_optimizer(new_lr, model)\n",
    "    \n",
    "    #train_loader, valid_loader = get_loaders(epoch)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1,num_workers=1)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1,num_workers=1)\n",
    "\n",
    "    # train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # validate\n",
    "    valid_loss, pos_loss, neg_loss, pos_mean, neg_mean = valid_epoch(model, valid_loader)\n",
    "    #valid_loss = 0\n",
    "    #pos_loss  = 0\n",
    "    #neg_loss = 0\n",
    "    #pos_mean = 0\n",
    "    #neg_mean = 0\n",
    "    \n",
    "    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, valid loss: {(valid_loss):.5f}'\n",
    "    print(content)\n",
    "    content = f'pos loss: {(pos_loss):.5f}, neg loss: {(neg_loss):.5f}, pos mean: {(pos_mean):.5f}, neg mean {(neg_mean):.5f}'\n",
    "    print(content)\n",
    "    master_train_loss.append(train_loss)\n",
    "    master_valid_loss.append(valid_loss)\n",
    "    \n",
    "    # save loss data and model weights\n",
    "    #with open('train_loss.pkl', 'wb') as f:\n",
    "        #pickle.dump(master_train_loss, f)\n",
    "    #with open('valid_loss.pkl', 'wb') as f:\n",
    "        #pickle.dump(master_valid_loss, f)\n",
    "        \n",
    "        #torch.save(model.state_dict(), 'model-resnext-50-{}.pth'.format(epoch))\n",
    "        #best_valid_loss = valid_loss\n",
    "    \n",
    "    epoch += 1\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ac465b0a2d0>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'model-efficientb0-lstm2.pth')\n",
    "plt.plot(master_train_loss[:], label='train')\n",
    "plt.plot(master_valid_loss[:], label='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_loss.pkl', 'wb') as f:\n",
    "    pickle.dump(master_train_loss, f)\n",
    "with open('valid_loss.pkl', 'wb') as f:\n",
    "    pickle.dump(master_valid_loss, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
